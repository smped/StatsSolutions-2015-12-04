---
title: "An Advanced Dataset"
author: "Steve Pederson"
date: "4 December 2015"
output: ioslides_presentation
---

## SNP Data

- In the `data` folder you'll find some SNP genotypes as `snps.csv`

- We'll write a function using the above to:
    + Collect each genotype into a contingency table
    + Run a Fisher Test on each table
    + Find any significant SNPs
    
## SNP Data

Load the file `snps.csv` as the *R* object `snps`

```{r, results='hide'}
library(readr)
snpFile <- file.path("data", "snps.csv")
file.exists(snpFile)
snps <- read_csv(snpFile)
```

## SNP Data

### What is a contingency table?

```{r}
table(snps[,1:2]) 
```

## SNP Data

1. The Fisher Test -
    + *Tests if the overall allele distribution varies across the populations*

```{r, message=FALSE}
library(dplyr)
table(snps[,1:2]) %>% fisher.test()
```

## SNP Data

2. Our output needs to have:
    + The SNP identifier
    + The `A` allele frequency in both populations
    + The *p*-value from `fisher.test()`
    
The output should be a `data_frame` for each SNP

```{r, echo=FALSE}
alleleTable <- table(snps[,1:2])
freqs <- apply(alleleTable, 
               MARGIN = 1,
               FUN = function(x) {
                 (x[1] + x[2]/2) / sum(x)
                 })
fisherRes <- fisher.test(alleleTable)
snpResults <- data_frame(SNP = "SNP1",
                         Control = freqs["Control"],
                         Treat = freqs["Treat"],
                         p.value = fisherRes$p.value)
snpResults
```

## How could we do this as a function?

## Here's my function

```{r}
runFisher <- function(SNP, data){
  alleleTable <- table(data[c("Population", SNP)])
  freqs <- apply(alleleTable, MARGIN = 1,
                 function(x) (x[1] + 0.5*x[2]) / sum(x))
  data_frame(SNP = SNP,
             Control = freqs["Control"],
             Treat = freqs["Treat"],
             p.value = fisher.test(alleleTable)$p.value)
}
runFisher("SNP1", snps)
```

## Running the function

We can now run the function, remembering to exclude the Population column from the list of SNPs

```{r}
allSnps <- colnames(snps)[-1]
snpResults <- lapply(allSnps, 
                     runFisher, 
                     data = snps) 
```

Now we have a list of data frames as our results

## Running the function

We could throw in the `magrittr` & some `dplyr` sugar

```{r, message=FALSE, warning=FALSE}
snpResults <- allSnps %>%
  lapply(runFisher, data = snps) %>%
  bind_rows() %>%
  mutate(adjP = p.adjust(p.value, 
                        method="bonferroni")) %>%
  arrange(p.value)
```

## And the results

```{r}
library(knitr)
format(snpResults[1:5,], digits=3) %>%
  kable
```

## Getting serious

This was a small dataset of `r ncol(snps)-1` SNPs

We also only ran one type of test

*What if we had 500,000 SNPs?*

The speed of a function can become important.

## Getting serious

Try this to see how fast the function was

```{r}
system.time(lapply(allSnps, runFisher, data=snps))
```

The final value is the total time taken by the process (in seconds)

## Getting serious

We can use the parallel version of `lapply()`: `mclapply()`

```{r}
library(parallel)
system.time(mclapply(allSnps, 
                     runFisher, 
                     data = snps,
                     mc.cores = 3))
```

## Getting Serious

- Using 3 cores on my laptop cut the time in half!

- Some time was sacrificed setting up the threads

- A recent project using parallel took a LD analysis from > 2 weeks to 2 days
    + We used 20 cores

## `parallel`

An alternative which gives you more control is to use the package `parallel`

- Based on the package `snow`

The idea is:

1. Set up a cluster (can be on your PC) 
2. Load any required packages on each node 
3. Run the process

## `parallel`

Here's the above analysis run using `parallel`

```{r, message=FALSE, warning=FALSE, results='hide'}
library(parallel)
nNodes <- 3
cl <- makePSOCKcluster(rep("localhost", nNodes))
clusterEvalQ(cl, library(dplyr))
snpResults <- parLapply(cl, #NB: `cl` goes first!
                        allSnps,
                        runFisher,
                        data = snps) %>%
  bind_rows() %>%
  mutate(adjP =p.adjust(p.value, 
                        method="bonferroni")) %>%
  arrange(p.value)
stopCluster(cl)
```

## `snow`

Was the original cross-platform package

Even more control over each node

1. Set up a cluster (can be on your PC)

2. Load any required packages on each node

3. Export any functions \& objects to each node

4. Run the process

## `snow`

```{r, message=FALSE, warning=FALSE}
library(snow)
nNodes <- 3
cl <- makeSOCKcluster(rep("localhost", nNodes))
splitSnps <- clusterSplit(cl, allSnps)
for (i in 1:nNodes) {
  clusterExport(cl[i], c("i", "splitSnps"))
  clusterEvalQ(cl[i], splitSnps <- splitSnps[[i]])
  }
clusterEvalQ(cl, splitSnps[1:3])
stopCluster(cl)
```


## `snow`

- There is always overhead setting up the cluster

- The cluster **always** needs to be stopped when you're done with it!
    + Occasionally won't shut down correctly

- `clusterApply()` is equivalent to `parLapply()`

- There is also a *load balancing* version,`clusterApplyLB()`